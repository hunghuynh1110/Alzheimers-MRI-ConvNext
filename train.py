# -*- coding: utf-8 -*-
"""notetrain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hJ0CT9myMA9XpMwpq7uvetrw1zNkjBWu
"""

EPOCHS = 150
DATA_ROOTs = "/content/data/AD_NC"

# 1. G·∫ÆN K·∫æT GOOGLE DRIVE
# B∆Ø·ªöC N√ÄY B·∫ÆT BU·ªòC ƒë·ªÉ Colab th·∫•y file tr√™n Drive c·ªßa b·∫°n.
from google.colab import drive
drive.mount('/content/gdrive')

# --- THAY ƒê·ªîI ƒê∆Ø·ªúNG D·∫™N T·∫†I ƒê√ÇY ---
# ƒê·ªïi t√™n v√† ƒë∆∞·ªùng d·∫´n file zip project (B√¢y gi·ªù l√† 'Pattern.zip')
# L∆ØU √ù: T√™n file tr√™n Drive c√≥ ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng. Ch√∫ng ta s·∫Ω th·ª≠ c·∫£ hai tr∆∞·ªùng h·ª£p.
project_zip_path_caps = '/content/gdrive/MyDrive/Pattern.zip'
project_zip_path_lower = '/content/gdrive/MyDrive/pattern.zip'
# ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu (Kh√¥ng thay ƒë·ªïi)
data_zip_path = '/content/gdrive/MyDrive/AD_NC.zip'

# 2. GI·∫¢I N√âN M√É NGU·ªíN V√Ä MODEL (file Pattern.zip)
# Gi·∫£i n√©n v√†o th∆∞ m·ª•c hi·ªán t·∫°i c·ªßa Colab (/content/)
# Sau khi gi·∫£i n√©n, c√°c file .ipynb, .py, .pth s·∫Ω n·∫±m trong /content/
print("Gi·∫£i n√©n m√£ ngu·ªìn v√† model...")

# Th·ª≠ gi·∫£i n√©n v·ªõi ƒë∆∞·ªùng d·∫´n vi·∫øt hoa (Pattern.zip)
unzip_command = f'unzip -q "{project_zip_path_caps}" -d /content/'
result = !{unzip_command}

# N·∫øu l·ªói 'cannot find', th·ª≠ v·ªõi ƒë∆∞·ªùng d·∫´n vi·∫øt th∆∞·ªùng (pattern.zip)
if "cannot find" in str(result):
    print("Kh√¥ng t√¨m th·∫•y 'Pattern.zip'. Th·ª≠ t√¨m 'pattern.zip'...")
    unzip_command = f'unzip -q "{project_zip_path_lower}" -d /content/'
    result = !{unzip_command}

# 3. GI·∫¢I N√âN D·ªÆ LI·ªÜU H√åNH ·∫¢NH (file AD_NC.zip)
# Vi·ªác n√†y ƒë√£ th√†nh c√¥ng, ch·ªâ c·∫ßn gi·ªØ nguy√™n
print("Gi·∫£i n√©n d·ªØ li·ªáu h√¨nh ·∫£nh...")
!unzip -q "{data_zip_path}" -d /content/data

# X√ìA TH∆Ø M·ª§C KH√îNG C·∫¶N THI·∫æT __MACOSX (n·∫øu c√≥)
!rm -rf /content/data/__MACOSX

# 4. KI·ªÇM TRA FILE ƒê√É T·ªíN T·∫†I CH∆ØA
print("Ki·ªÉm tra c√°c file quan tr·ªçng:")
# Ki·ªÉm tra file m√£ ngu·ªìn ch√≠nh
!ls /content/notetrain.ipynb
# Ki·ªÉm tra th∆∞ m·ª•c d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c gi·∫£i n√©n
!ls /content/data/AD_NC

import os

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights

import matplotlib.pyplot as plt

torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False


from dataset import get_loaders
from constants import (
    DEVICE_MPS, DEVICE_CUDA, DEVICE_CPU,
    BATCH_SIZE, LR,
    DATA_ROOT,
    NUM_CLASSES, DROP_PATH_RATE
)
# Device setup
if torch.backends.mps.is_available():
    DEVICE = DEVICE_MPS
elif torch.cuda.is_available():
    DEVICE = DEVICE_CUDA
else:
    DEVICE = DEVICE_CPU

from sklearn.metrics import roc_auc_score

print("Current working directory:", os.getcwd())
print(f"Using device: {DEVICE}")

# Load data
train_loader, val_loader, test_loader = get_loaders(
    data_root  =  DATA_ROOTs,
    batch_size = BATCH_SIZE
)
print(f"Train images: {len(train_loader.dataset)} | "
    f"Val images:   {len(val_loader.dataset)} | "
    f"Test images:  {len(test_loader.dataset)}")
# Model setup
model = ConvNeXtMRI(
    in_chans=3,
    num_classes=NUM_CLASSES,
    depths=[2, 2, 4, 2],
    dims=[48, 96, 192, 384],
    drop_path_rate=DROP_PATH_RATE
).to(DEVICE)

class_weights = torch.tensor([1.1, 0.9], dtype=torch.float32).to(DEVICE)
criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)
optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)


from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR

warmup = LinearLR(optimizer, start_factor=0.1, total_iters=3)
cosine = CosineAnnealingLR(optimizer, T_max=EPOCHS - 3, eta_min=1e-6)

scheduler = SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[3])

# (Continue with training loop‚Ä¶)
best_val_acc = 0.0
history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
epochs = range(1, EPOCHS + 1)

lrs = []  # track learning rate per epoch

scaler = torch.amp.GradScaler('cuda')

for epoch in range(1, EPOCHS + 1):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for batch_idx, (inputs, labels) in enumerate(train_loader, start=1):
        inputs = inputs.to(DEVICE)
        labels = labels.to(DEVICE)
        optimizer.zero_grad()

        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        if batch_idx % 50 == 0:
            print(f'Epoch {epoch} | Batch {batch_idx}/{len(train_loader)} | '
                  f'Loss: {running_loss / batch_idx:.4f} | '
                  f'Acc: {100 * correct / total:.2f}%')

    # === End of training epoch ===
    train_loss = running_loss / len(train_loader)
    train_acc  = 100 * correct / total

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    val_preds, val_labels_np = [], []

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(DEVICE)
            labels = labels.to(DEVICE)
            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = outputs.max(1)
            val_total += labels.size(0)
            val_correct += predicted.eq(labels).sum().item()
            val_preds.extend(predicted.cpu().numpy())
            val_labels_np.extend(labels.cpu().numpy())

    val_loss = val_loss / len(val_loader)
    val_acc  = 100 * val_correct / val_total

    print(f'Epoch {epoch} completed: '
          f'Train Loss {train_loss:.4f}, Train Acc {train_acc:.2f}%, '
          f'Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%')

    # === Additional validation metrics ===
    from sklearn.metrics import precision_score, recall_score, f1_score

    val_precision = precision_score(val_labels_np, val_preds, average='macro')
    val_recall = recall_score(val_labels_np, val_preds, average='macro')
    val_f1 = f1_score(val_labels_np, val_preds, average='macro')

    print(f"Val Precision: {val_precision:.3f}, Recall: {val_recall:.3f}, F1: {val_f1:.3f}")



    # === üß™ Test Evaluation (no learning, just monitoring) ===
    from sklearn.metrics import roc_auc_score

    model.eval()
    all_probs, all_labels = [], []
    test_correct, test_total = 0, 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability of class NC
            _, predicted = outputs.max(1)
            test_total += labels.size(0)
            test_correct += predicted.eq(labels).sum().item()

            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Compute metrics
    test_acc = 100 * test_correct / test_total
    test_auc = roc_auc_score(all_labels, all_probs)

    print(f"üß† [TEST] Epoch {epoch}: Acc={test_acc:.2f}%, AUC={test_auc:.3f}")

    # Log results
    with open('training_log.txt', 'a') as f:
        f.write(f"[TEST] Epoch {epoch}: Acc={test_acc:.2f}%, AUC={test_auc:.3f}\n")

    # # Optional: Stop training early if test AUC ‚â• 0.8
    # if test_auc >= 0.8:
    #     print(f"üéØ Early stop! Test AUC={test_auc:.3f} ‚â• 0.8")
    #     break



    # --- OPTIONAL: Validation threshold tuning ---
    if epoch % 5 == 0 or epoch == EPOCHS:
        all_val_probs, all_val_labels = [], []
        model.eval()
        with torch.no_grad():
            for x, y in val_loader:
                x = x.to(DEVICE)
                probs = torch.softmax(model(x), dim=1)[:, 0]  # prob(AD)
                all_val_probs.extend(probs.cpu().numpy())
                all_val_labels.extend(y.cpu().numpy())

        import numpy as np
        best_th, best_f1 = 0.5, 0
        for th in np.linspace(0.2, 0.8, 61):
            preds = (np.array(all_val_probs) >= th).astype(int)
            f1 = f1_score(all_val_labels, preds, pos_label=0)
            if f1 > best_f1:
                best_f1, best_th = f1, th

        print(f"üîß Best AD threshold (epoch {epoch}): {best_th:.3f} (Val F1={best_f1:.3f})")
        with open("best_threshold.txt", "w") as f:
            f.write(f"{best_th:.3f}\n")

    # === Record metrics and save ===
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['train_acc'].append(train_acc)
    history['val_acc'].append(val_acc)

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_model.pth')
        print(f'‚úÖ New best model saved! (Val Acc: {val_acc:.2f}%)')

    with open('training_log.txt', 'a') as f:
        f.write(f'Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, '
                f'Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%, '
                f'Precision={val_precision:.3f}, Recall={val_recall:.3f}, F1={val_f1:.3f}\n')

    scheduler.step()
    current_lr = optimizer.param_groups[0]['lr']
    lrs.append(current_lr)
    print(f"Current LR after epoch {epoch}: {current_lr:.6f}")

"""# Plot training curves"""

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(epochs, history['train_loss'], label='Train Loss')
plt.plot(epochs, history['val_loss'], label='Val Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch'); plt.ylabel('Loss')
plt.legend()
plt.subplot(1,2,2)
plt.plot(epochs, history['train_acc'], label='Train Acc')
plt.plot(epochs, history['val_acc'], label='Val Acc')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)')
plt.legend()
plt.tight_layout()
plt.savefig('training_curves.png', dpi=150)
plt.show()
print('üìä Training curves saved as training_curves.png')

import pandas as pd

# Save training metrics as CSV for record-keeping
pd.DataFrame(history).to_csv("training_history.csv", index=False)
print("üìÅ Training history saved as training_history.csv")

"""Plot LR curve"""

plt.figure()
plt.plot(range(1, len(lrs) + 1), lrs, marker='o')
plt.title('Learning Rate Schedule')
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.grid(True)
plt.savefig('lr_schedule.png', dpi=150)
plt.show()
print('üìà Learning rate schedule saved as lr_schedule.png')

# === Load best model (prefer the test-pass version if available) ===
if os.path.exists('best_model_test_acc.pth'):
    model.load_state_dict(torch.load('best_model_test_acc.pth'))
    print("‚úÖ Loaded model: best_model_test_acc.pth")
else:
    model.load_state_dict(torch.load('best_model.pth'))
    print("‚úÖ Loaded model: best_model.pth (best validation model)")
    
model.eval()
test_correct, test_total = 0, 0
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
        outputs = model(inputs)
        _, predicted = outputs.max(1)
        test_total += labels.size(0)
        test_correct += predicted.eq(labels).sum().item()

test_acc = 100 * test_correct / test_total
print(f"üß† Final Test Accuracy: {test_acc:.2f}%")

num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"üß© Model parameters: {num_params/1e6:.2f}M")

# === ROC Curve and AUC Score ===
from sklearn.metrics import roc_curve, auc
import numpy as np
import matplotlib.pyplot as plt

print("\nüìà Generating ROC Curve...")

# Compute predicted probabilities for the positive class (NC = 1) and collect labels
all_probs = []
all_labels = []  # Initialize all_labels list
with torch.no_grad():
    for inputs, labels in test_loader:  # Iterate through test_loader to get inputs and labels
        inputs = inputs.to(DEVICE)
        # labels = labels.to(DEVICE) # Labels don't need to be on the device for calculating ROC
        outputs = model(inputs)
        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1 (NC)
        all_probs.extend(probs.cpu().numpy())
        all_labels.extend(labels.cpu().numpy()) # Collect labels

# Convert to numpy arrays
all_probs = np.array(all_probs)
all_labels = np.array(all_labels)

# Compute ROC curve and AUC
fpr, tpr, _ = roc_curve(all_labels, all_probs)
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2,
         label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.savefig('roc_curve.png', dpi=150)
plt.show()

print(f"‚úÖ ROC curve saved as roc_curve.png | AUC = {roc_auc:.3f}")

# Optionally record the AUC value in your training log
with open("training_log.txt", "a") as f:
    f.write(f"AUC Score: {roc_auc:.4f}\n")

from sklearn.metrics import roc_curve, classification_report, balanced_accuracy_score, f1_score
import numpy as np
import pandas as pd
import torch

print("\nüìä Generating detailed classification metrics...")

# === Collect true labels and predicted probabilities ===
all_labels = []
all_scores = []   # probabilities for AD class

model.eval()
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = inputs.to(DEVICE)
        outputs = model(inputs)
        probs = torch.softmax(outputs, dim=1)[:, 1]  # probability of AD
        all_scores.extend(probs.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

all_scores = np.array(all_scores)
all_labels = np.array(all_labels)

# === Find optimal threshold from ROC curve ===
fpr, tpr, thresholds = roc_curve(all_labels, all_scores)
best_idx = (tpr - fpr).argmax()
best_thr = thresholds[best_idx]
print(f"üî• Optimal decision threshold: {best_thr:.3f}")

# === Apply threshold ===
all_preds = (all_scores > best_thr).astype(int)

# === Evaluate using balanced metrics ===
report = classification_report(
    all_labels,
    all_preds,
    target_names=['AD', 'NC'],
    output_dict=True
)
report_df = pd.DataFrame(report).transpose()

balanced_acc = balanced_accuracy_score(all_labels, all_preds)
macro_f1 = f1_score(all_labels, all_preds, average='macro')
print(f"\nBalanced Accuracy: {balanced_acc:.4f}")
print(f"Macro F1: {macro_f1:.4f}")
print(report_df)

# === Save to CSV for report inclusion ===
report_df.to_csv('classification_report.csv', index=True)
print("‚úÖ Class-wise metrics saved as classification_report.csv")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(all_labels, all_preds)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['AD', 'NC'], yticklabels=['AD', 'NC'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.savefig("confusion_matrix.png", dpi=150)
plt.show()
print("‚úÖ confusion_matrix.png saved successfully.")

# === Balanced Example Predictions Visualization (4 AD + 4 NC) ===
import random
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

print("\nüß† Generating balanced example predictions visualization...")

# Separate AD and NC indices in test dataset
ad_indices = [i for i, (_, label) in enumerate(test_loader.dataset.samples) if label == 0]
nc_indices = [i for i, (_, label) in enumerate(test_loader.dataset.samples) if label == 1]

# Randomly select 4 from each class
sample_indices = random.sample(ad_indices, 4) + random.sample(nc_indices, 4)
random.shuffle(sample_indices)

plt.figure(figsize=(14, 6))
for i, idx in enumerate(sample_indices):
    img_path, true_label = test_loader.dataset.samples[idx]
    img = Image.open(img_path).convert('RGB')

    # Apply same preprocessing as training
    preprocess = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])
    img_tensor = preprocess(img).unsqueeze(0).to(DEVICE)

    # Predict
    with torch.no_grad():
        output = model(img_tensor)
        pred = output.argmax(dim=1).item()

    # Plot
    plt.subplot(2, 4, i + 1)
    plt.imshow(Image.open(img_path))
    plt.title(f"True: {['AD','NC'][true_label]}\nPred: {['AD','NC'][pred]}")
    plt.axis('off')

plt.tight_layout()
plt.savefig("example_predictions_balanced.png", dpi=150)
plt.show()
print("‚úÖ Balanced example predictions saved as example_predictions_balanced.png")

# === ‚úÖ AUTOMATIC BACKUP TO GOOGLE DRIVE (no manual download needed) ===

from google.colab import drive
import shutil
import os

# 1Ô∏è‚É£ Mount Google Drive
drive.mount('/content/drive')

# 2Ô∏è‚É£ Choose where to save your results on Drive
SAVE_DIR = '/content/drive/MyDrive/convnext_results_backup'
os.makedirs(SAVE_DIR, exist_ok=True)

# 3Ô∏è‚É£ List all result files, including the new ones
result_files = [
    'best_model.pth',
    'confusion_matrix.png',
    'lr_schedule.png',
    'training_curves.png',
    'training_history.csv',
    'training_log.txt',
    'roc_curve.png',                   # ‚úÖ NEW
    'classification_report.csv',       # ‚úÖ NEW
    'example_predictions.png',         # ‚úÖ Optional (if you add the visualization)
    'results_backup.zip'               # Optional archive
]

# 4Ô∏è‚É£ Copy each file to Drive if it exists
for file in result_files:
    if os.path.exists(file):
        dest_path = os.path.join(SAVE_DIR, os.path.basename(file))
        shutil.copy(file, dest_path)
        print(f"‚úÖ Saved to Drive: {dest_path}")
    else:
        print(f"‚ö†Ô∏è File not found: {file}")

print("\nüì¶ All result files have been backed up to Google Drive successfully!")
print(f"üìÇ Drive folder: {SAVE_DIR}")

from google.colab import runtime
runtime.unassign()
